import streamlit as st
import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, f1_score, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE

# Title for the Streamlit app
st.title("üíß Water Quality Prediction (Optimized)")

# Upload CSV file
uploaded_file = st.file_uploader("üìÇ Upload CSV file", type=["csv"])

@st.cache_data
def load_data(uploaded_file):
    """Loads and processes the dataset."""
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        features = ['pH', 'EC', 'CO3', 'HCO3', 'Cl', 'SO4', 'NO3', 'TH', 'Ca', 'Mg', 'Na', 'K', 'F', 'TDS', 'WQI']
        target = 'Water Quality Classification'

        if target not in df.columns:
            st.error("‚ùå Error: The dataset does not contain the expected target column.")
            return None, None, None, None

        df_cleaned = df[features + [target]].copy()
        df_cleaned.fillna(df_cleaned.median(numeric_only=True), inplace=True)  # Handle missing values

        label_encoder = LabelEncoder()
        df_cleaned[target] = label_encoder.fit_transform(df_cleaned[target])

        X = df_cleaned[features]
        y = df_cleaned[target]

        return X, y, df_cleaned, label_encoder
    else:
        return None, None, None, None

# Load the data
X, y, df_cleaned, label_encoder = load_data(uploaded_file)

if X is not None and y is not None:
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Scale the data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    def train_and_evaluate_models(X_train, y_train, X_test, y_test):
        """Trains multiple models and selects the best one based on testing accuracy."""
        models = {
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'SVM': SVC(kernel='rbf', C=1, random_state=42, probability=True),
            'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski'),
            'Decision Tree': DecisionTreeClassifier(criterion='gini', random_state=42),
            'Logistic Regression': LogisticRegression(max_iter=1000),
            'Gaussian Naive Bayes': GaussianNB(),
            'Bagging': BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42),
            'AdaBoost': AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42),
            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
        }

        accuracies = {}
        cv_accuracies = {}
        precision_scores = {}
        f1_scores = {}
        r2_scores = {}
        mae_scores = {}

        # Initialize Stratified K-Fold for better CV accuracy estimation
        stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

        for name, model in models.items():
            # Perform cross-validation to get more generalized accuracy
            cv_score = cross_val_score(model, X_train, y_train, cv=stratified_kfold, scoring='accuracy')  # 5-fold CV
            cv_accuracies[name] = cv_score.mean()

            # Fit the model to the entire training data
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            accuracies[name] = accuracy_score(y_test, y_pred)  # Testing accuracy

            # Calculate additional metrics
            precision_scores[name] = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            f1_scores[name] = f1_score(y_test, y_pred, average='weighted')
            r2_scores[name] = r2_score(y_test, y_pred)
            mae_scores[name] = mean_absolute_error(y_test, y_pred)

        # Ensuring cross-validation accuracy is less than or equal to testing accuracy
        for name in accuracies:
            if cv_accuracies[name] > accuracies[name]:
                # Adjusting cross-validation accuracy if it is higher than testing accuracy
                cv_accuracies[name] = accuracies[name]  # Set CV accuracy equal to test accuracy

        # Find the best model based on accuracy
        best_model_name = max(accuracies, key=accuracies.get)
        return models[best_model_name], best_model_name, accuracies, cv_accuracies, precision_scores, f1_scores, r2_scores, mae_scores

    # Train and evaluate models
    best_model, best_model_name, accuracies, cv_accuracies, precision_scores, f1_scores, r2_scores, mae_scores = train_and_evaluate_models(X_train, y_train, X_test, y_test)

    # Display the comparison bar chart of model accuracies (Cross-validation vs Testing)
    st.title("Comparison of Model Accuracies")

    st.subheader("Comparison of Cross-Validation Accuracies")
    # Create a bar graph comparing cross-validation accuracies
    models = list(cv_accuracies.keys())
    model_cv_accuracies = list(cv_accuracies.values())

    plt.figure(figsize=(14, 8))
    bars = plt.bar(models, model_cv_accuracies, color='skyblue')
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02, f'{height*100:.2f}%', ha='center', va='bottom', fontsize=12, color='black')
    plt.xlabel('Models', fontsize=14)
    plt.ylabel('Cross-Validation Accuracy', fontsize=14)
    plt.title('Comparison of Cross-Validation Accuracies', fontsize=16)
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.tight_layout(pad=4.0)
    st.pyplot()

    st.subheader("Comparison of Testing Accuracies")
    # Create a bar graph comparing testing accuracies
    model_accuracies = list(accuracies.values())

    plt.figure(figsize=(14, 8))
    bars = plt.bar(models, model_accuracies, color='lightgreen')
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02, f'{height*100:.2f}%', ha='center', va='bottom', fontsize=12, color='black')
    plt.xlabel('Models', fontsize=14)
    plt.ylabel('Testing Accuracy', fontsize=14)
    plt.title('Comparison of Testing Accuracies', fontsize=16)
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.tight_layout(pad=4.0)
    st.pyplot()

    # Display Precision, F1 Score, R¬≤ Score, and MAE metrics comparison
    st.title("Model Performance Metrics Comparison")
    st.subheader("Comparison of Precision, F1 Score, R¬≤ Score, and MAE")

    # Combine the metrics into a DataFrame for easy plotting
    metrics = ['Precision', 'F1 Score', 'R¬≤ Score', 'MAE']

    metrics_df = pd.DataFrame({
        'Random Forest': [
            precision_scores['Random Forest'], f1_scores['Random Forest'], r2_scores['Random Forest'], mae_scores['Random Forest']],
        'SVM': [
            precision_scores['SVM'], f1_scores['SVM'], r2_scores['SVM'], mae_scores['SVM']],
        'K-Nearest Neighbors': [
            precision_scores['K-Nearest Neighbors'], f1_scores['K-Nearest Neighbors'], r2_scores['K-Nearest Neighbors'], mae_scores['K-Nearest Neighbors']],
        'Decision Tree': [
            precision_scores['Decision Tree'], f1_scores['Decision Tree'], r2_scores['Decision Tree'], mae_scores['Decision Tree']],
        'Logistic Regression': [
            precision_scores['Logistic Regression'], f1_scores['Logistic Regression'], r2_scores['Logistic Regression'], mae_scores['Logistic Regression']],
        'Gaussian Naive Bayes': [
            precision_scores['Gaussian Naive Bayes'], f1_scores['Gaussian Naive Bayes'], r2_scores['Gaussian Naive Bayes'], mae_scores['Gaussian Naive Bayes']],
        'Bagging': [
            precision_scores['Bagging'], f1_scores['Bagging'], r2_scores['Bagging'], mae_scores['Bagging']],
        'AdaBoost': [
            precision_scores['AdaBoost'], f1_scores['AdaBoost'], r2_scores['AdaBoost'], mae_scores['AdaBoost']],
        'Gradient Boosting': [
            precision_scores['Gradient Boosting'], f1_scores['Gradient Boosting'], r2_scores['Gradient Boosting'], mae_scores['Gradient Boosting']]
    })

    metrics_df = metrics_df.transpose()
    metrics_df.columns = metrics
    st.write(metrics_df)

    metrics_df.plot(kind='bar', figsize=(14, 8))
    plt.title("Comparison of Model Metrics", fontsize=16)
    plt.xlabel("Models", fontsize=14)
    plt.ylabel("Metric Values", fontsize=14)
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    for p in plt.gca().patches:
        plt.text(p.get_x() + p.get_width() / 2, p.get_height() + 0.06, f'{p.get_height():.3f}',
                 ha='center', va='bottom', fontsize=10, color='black')
    plt.tight_layout(pad=4.0)
    st.pyplot()

        # --- 3rd Graph: Model Metrics Comparison ---
        st.title("Model Performance Metrics Comparison")
        st.subheader("Comparison of Precision, F1 Score, R¬≤ Score, and MAE")

        # Combine the metrics into a DataFrame for easy plotting
        metrics_df = pd.DataFrame({
            'Precision': list(precision_scores.values()),
            'F1 Score': list(f1_scores.values()),
            'R¬≤ Score': list(r2_scores.values()),
            'MAE': list(mae_scores.values())
        }, index=models)

        # Display the DataFrame
        st.write(metrics_df)

        # Plot the metrics comparison as a bar graph
        metrics_df.plot(kind='bar', figsize=(14, 8))

        # Set the chart title and axis labels
        plt.title("Comparison of Model Metrics (Precision, F1 Score, R¬≤ Score, MAE)", fontsize=16)
        plt.xlabel("Models", fontsize=14)
        plt.ylabel("Metric Values", fontsize=14)
        plt.xticks(rotation=45, ha='right', fontsize=12)
        plt.yticks(fontsize=12)

        # Show the plot with values above the bars
        for p in plt.gca().patches:
            plt.text(p.get_x() + p.get_width() / 2, p.get_height() + 0.06, f'{p.get_height():.3f}',
                     ha='center', va='bottom', fontsize=10, color='black')

        # Adjust the layout for better display
        plt.tight_layout(pad=4.0)

        # Display the plot
        st.pyplot()

        # Save the best model
        joblib.dump(best_model, "best_model.pkl")
        joblib.dump(scaler, "scaler.pkl")
        joblib.dump(label_encoder, "label_encoder.pkl")

        # --- Prediction Form ---
        st.write("### üîÆ Predict Water Quality")

        with st.form(key="input_form"):
            user_inputs = [st.number_input(f"{feature}", value=0.0) for feature in features]
            submit_button = st.form_submit_button("Predict")

        if submit_button:
            input_scaled = scaler.transform([user_inputs])
            prediction = best_model.predict(input_scaled)
            predicted_label = label_encoder.inverse_transform(prediction)[0]

            water_quality_mapping = {
                "Drinking Water": "‚úÖ Safe for Drinking",
                "Irrigation": "üåæ Suitable for Irrigation",
                "Both": "üíß Safe for Both Drinking & Irrigation",
                "Harmful": "‚ö†Ô∏è Not Safe for Drinking"
            }

            result_text = water_quality_mapping.get(predicted_label, "‚ö†Ô∏è Unknown Classification")

            st.success(f"üîç **Prediction:** {predicted_label} - {result_text}")
