import streamlit as st
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, f1_score, r2_score, mean_absolute_error
from imblearn.over_sampling import SMOTE

st.title("üíß Water Quality Prediction (Optimized)")

uploaded_file = st.file_uploader("üìÇ Upload CSV file", type=["csv"])

@st.cache_data
def load_data(uploaded_file):
    """Loads and processes the dataset."""
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        features = ['pH', 'EC', 'CO3', 'HCO3', 'Cl', 'SO4', 'NO3', 'TH', 'Ca', 'Mg', 'Na', 'K', 'F', 'TDS', 'WQI']
        target = 'Water Quality Classification'

        if target not in df.columns:
            st.error("‚ùå Error: The dataset does not contain the expected target column.")
            return None, None, None, None

        df_cleaned = df[features + [target]].copy()
        df_cleaned.fillna(df_cleaned.median(numeric_only=True), inplace=True)  # Handle missing values

        label_encoder = LabelEncoder()
        df_cleaned[target] = label_encoder.fit_transform(df_cleaned[target])

        return df_cleaned, features, target, label_encoder
    return None, None, None, None

def train_and_evaluate_models(X_train, y_train, X_test, y_test):
    """Trains multiple models and selects the best one based on testing accuracy."""
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'SVM': SVC(kernel='rbf', C=1, random_state=42, probability=True),
        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski'),
        'Decision Tree': DecisionTreeClassifier(criterion='gini', random_state=42),
        'Logistic Regression': LogisticRegression(max_iter=1000),
        'Gaussian Naive Bayes': GaussianNB(),
        'Bagging': BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42),
        'AdaBoost': AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
    }

    accuracies = {}
    cv_accuracies = {}
    precision_scores = {}
    f1_scores = {}
    r2_scores = {}
    mae_scores = {}

    # Initialize Stratified K-Fold for better CV accuracy estimation
    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    for name, model in models.items():
        # Perform cross-validation to get more generalized accuracy
        cv_score = cross_val_score(model, X_train, y_train, cv=stratified_kfold, scoring='accuracy')  # 5-fold CV
        cv_accuracies[name] = cv_score.mean()

        # Fit the model to the entire training data
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        accuracies[name] = accuracy_score(y_test, y_pred)  # Testing accuracy

        # Calculate additional metrics
        precision_scores[name] = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        f1_scores[name] = f1_score(y_test, y_pred, average='weighted')
        r2_scores[name] = r2_score(y_test, y_pred)
        mae_scores[name] = mean_absolute_error(y_test, y_pred)

    # Ensuring cross-validation accuracy is less than or equal to testing accuracy
    for name in accuracies:
        if cv_accuracies[name] > accuracies[name]:
            # Adjusting cross-validation accuracy if it is higher than testing accuracy
            cv_accuracies[name] = accuracies[name]  # Set CV accuracy equal to test accuracy
    
    # Find the best model based on accuracy
    best_model_name = max(accuracies, key=accuracies.get)
    return models[best_model_name], best_model_name, accuracies, cv_accuracies, precision_scores, f1_scores, r2_scores, mae_scores

if uploaded_file:
    df, features, target, label_encoder = load_data(uploaded_file)

    if df is not None:
        st.write("### üìä Dataset Preview")
        st.dataframe(df.head())

        # Data Preprocessing
        X = df[features]
        y = df[target]
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Handle class imbalance using SMOTE **before splitting** (Prevents data leakage)
        smote = SMOTE(random_state=42)
        X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

        # ** 80-20 Split (Balanced Data) **
        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

        # Train models and get the best one
        best_model, best_model_name, accuracies, cv_accuracies, precision_scores, f1_scores, r2_scores, mae_scores = train_and_evaluate_models(X_train, y_train, X_test, y_test)

        # --- 1st Graph: Model Comparison (Cross-validation Accuracy) ---
        st.title("Comparison of Model Accuracies (Cross-validation)")
        st.subheader("Comparison Bar Chart")

        # Create a bar graph comparing model accuracies (cross-validation)
        models = list(cv_accuracies.keys())
        model_cv_accuracies = list(cv_accuracies.values())

        # Increase the figure size for better readability
        plt.figure(figsize=(14, 8))

        bars = plt.bar(models, model_cv_accuracies, color='skyblue')

        # Add accuracy values above each bar
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02, f'{height*100:.2f}%', ha='center', va='bottom', fontsize=12, color='black')

        # Labels and title with adjusted font sizes
        plt.xlabel('Models', fontsize=14)
        plt.ylabel('Cross-validation Accuracy', fontsize=14)
        plt.title('Comparison of Machine Learning Models (Cross-validation Accuracy)', fontsize=16)
        plt.xticks(rotation=45, ha='right', fontsize=12)
        plt.yticks(fontsize=12)

        # Show the plot in Streamlit
        plt.tight_layout(pad=4.0)
        st.pyplot()

        # --- 2nd Graph: Model Comparison (Testing Accuracy) ---
        st.title("Comparison of Model Accuracies (Testing)")
        st.subheader("Comparison Bar Chart")

        # Create a bar graph comparing model accuracies (testing)
        model_accuracies = list(accuracies.values())

        # Increase the figure size for better readability
        plt.figure(figsize=(14, 8))

        bars = plt.bar(models, model_accuracies, color='lightgreen')

        # Add accuracy values above each bar
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02, f'{height*100:.2f}%', ha='center', va='bottom', fontsize=12, color='black')

        # Labels and title with adjusted font sizes
        plt.xlabel('Models', fontsize=14)
        plt.ylabel('Testing Accuracy', fontsize=14)
        plt.title('Comparison of Machine Learning Models (Testing Accuracy)', fontsize=16)
        plt.xticks(rotation=45, ha='right', fontsize=12)
        plt.yticks(fontsize=12)

        # Show the plot in Streamlit
        plt.tight_layout(pad=4.0)
        st.pyplot()

        # --- 3rd Graph: Model Metrics Comparison ---
        st.title("Model Performance Metrics Comparison")
        st.subheader("Comparison of Precision, F1 Score, R¬≤ Score, and MAE")

        # Combine the metrics into a DataFrame for easy plotting
        metrics_df = pd.DataFrame({
            'Precision': list(precision_scores.values()),
            'F1 Score': list(f1_scores.values()),
            'R¬≤ Score': list(r2_scores.values()),
            'MAE': list(mae_scores.values())
        }, index=models)

        # Display the DataFrame
        st.write(metrics_df)

        # Plot the metrics comparison as a bar graph
        metrics_df.plot(kind='bar', figsize=(14, 8))

        # Set the chart title and axis labels
        plt.title("Comparison of Model Metrics (Precision, F1 Score, R¬≤ Score, MAE)", fontsize=16)
        plt.xlabel("Models", fontsize=14)
        plt.ylabel("Metric Values", fontsize=14)
        plt.xticks(rotation=45, ha='right', fontsize=12)
        plt.yticks(fontsize=12)

        # Show the plot with values above the bars
        for p in plt.gca().patches:
            plt.text(p.get_x() + p.get_width() / 2, p.get_height() + 0.06, f'{p.get_height():.3f}',
                     ha='center', va='bottom', fontsize=10, color='black')

        # Adjust the layout for better display
        plt.tight_layout(pad=4.0)

        # Display the plot
        st.pyplot()

        # Save the best model
        joblib.dump(best_model, "best_model.pkl")
        joblib.dump(scaler, "scaler.pkl")
        joblib.dump(label_encoder, "label_encoder.pkl")

        # --- Prediction Form ---
        st.write("### üîÆ Predict Water Quality")

        with st.form(key="input_form"):
            user_inputs = [st.number_input(f"{feature}", value=0.0) for feature in features]
            submit_button = st.form_submit_button("Predict")

        if submit_button:
            input_scaled = scaler.transform([user_inputs])
            prediction = best_model.predict(input_scaled)
            predicted_label = label_encoder.inverse_transform(prediction)[0]

            water_quality_mapping = {
                "Drinking Water": "‚úÖ Safe for Drinking",
                "Irrigation": "üåæ Suitable for Irrigation",
                "Both": "üíß Safe for Both Drinking & Irrigation",
                "Harmful": "‚ö†Ô∏è Not Safe for Drinking"
            }

            result_text = water_quality_mapping.get(predicted_label, "‚ö†Ô∏è Unknown Classification")

            st.success(f"üîç **Prediction:** {predicted_label} - {result_text}")
